# Overview

With all of the recent developments in methods for applied empirical micro, it can be difficult to keep everything organized. Should I try synthetic control? I have an instrument, but what tests should I run? How sensitive are my IV results to outliers or to weak instruments? What was the name of that RD test again, and why do some people bin the outcome first?

I put these diagrams and links together to help me keep some of this straight. These aren't comprehensive, but I think they serve as a decent reference for the key things to keep in mind, standard tests to consider, and alternative estimators \(when relevant\). I'm updating these things constantly as I find new information and correct my own misunderstanding. If you see something awry, please [let me know](./#contact)!

Now, the goal with all of this is **NOT** to teach the statistics of any given estimator or research design.

![](.gitbook/assets/mathschoolofrock.gif)

The goal is to navigate all of the other stuff that you have to do before you can even rely on the results of such an estimation. Estimating something with RD, DD, or IV is one thing, but providing convincing evidence of a causal effect is a much bigger question \(and, I would argue, is the implicit goal of anyone using these methods anyway\).

