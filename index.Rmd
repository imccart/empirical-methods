--- 
title: "Navigating Empirical Methods"
author: "Ian McCarthy"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a working bookdown version of a workshop and future class, "Navigating Empirical Methods"
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Overview

With all of the recent developments in methods for applied empirical micro, it can be difficult to keep everything organized. Should I try synthetic control? I have an instrument, but what tests should I run? How sensitive are my IV results to outliers or to weak instruments? What was the name of that RD test again, and why do some people bin the outcome first?

I put these diagrams and links together to help me keep some of this straight. These aren't comprehensive, but I think they serve as a decent reference for the key things to keep in mind, standard tests to consider, and alternative estimators (when relevant). I'm updating these things constantly as I find new information and correct my own misunderstanding. If you see something awry, please [let me know](#contact)!

Now, the goal with all of this is **NOT** to teach the statistics of any given estimator or research design. 

<center>
<iframe src="https://giphy.com/embed/XyOrJljDNBEpa" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
</center>

The goal is to navigate all of the other stuff that you have to do before you can even rely on the results of such an estimation. Estimating something with RD, DD, or IV is one thing, but providing convincing evidence of a causal effect is a much bigger question (and, I would argue, is the implicit goal of anyone using these methods anyway).
